---
title: "Reading and writing data files"
description: >
  Learn how to read and write CSV, Parquet, and Feather files with `arrow` 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reading and writing data files}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `arrow` package provides functions for reading single data files in
several common formats. By default, calling any of these functions
returns an R `data.frame`. To return an Arrow `Table`, set argument
`as_data_frame = FALSE`.

-   `read_parquet()`: read a file in Parquet format
-   `read_feather()`: read a file in Feather format (the Apache Arrow
    IPC format)
-   `read_delim_arrow()`: read a delimited text file (default delimiter
    is comma)
-   `read_csv_arrow()`: read a comma-separated values (CSV) file
-   `read_tsv_arrow()`: read a tab-separated values (TSV) file
-   `read_json_arrow()`: read a JSON data file

For writing data to single files, the `arrow` package provides the
functions `write_parquet()`, `write_feather()`, and `write_csv_arrow()`.
These can be used with R `data.frame` and Arrow `Table` objects.

For example, let’s write the Star Wars characters data that’s included
in `dplyr` to a Parquet file, then read it back in. Parquet is a popular
choice for storing analytic data; it is optimized for reduced file sizes
and fast read performance, especially for column-based access patterns.
Parquet is widely supported by many tools and platforms.

First load the `arrow` and `dplyr` packages:

``` r
library(arrow, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
```

Then write the `data.frame` named `starwars` to a Parquet file at
`file_path`:

``` r
file_path <- tempfile()
write_parquet(starwars, file_path)
```

Then read the Parquet file into an R `data.frame` named `sw`:

``` r
sw <- read_parquet(file_path)
```

R object attributes are preserved when writing data to Parquet or
Feather files and when reading those files back into R. This enables
round-trip writing and reading of `sf::sf` objects, R `data.frame`s with
with `haven::labelled` columns, and `data.frame`s with other custom
attributes.

For reading and writing larger files or sets of multiple files, `arrow`
defines `Dataset` objects and provides the functions `open_dataset()`
and `write_dataset()`, which enable analysis and processing of
bigger-than-memory data, including the ability to partition data into
smaller chunks without loading the full data into memory. For examples
of these functions, see `vignette("dataset", package = "arrow")`.

All these functions can read and write files in the local filesystem or
in Amazon S3 (by passing S3 URIs beginning with `s3://`). For more
details, see `vignette("fs", package = "arrow")`



## FROM OLD GETTING STARTED

`arrow` provides some simple functions for using the Arrow C++ library to read and write files.
These functions are designed to drop into your normal R workflow
without requiring any knowledge of the Arrow C++ library
and use naming conventions and arguments that follow popular R packages, particularly `readr`.
The readers return `data.frame`s
(or if you use the `tibble` package, they will act like `tbl_df`s),
and the writers take `data.frame`s.

Importantly, `arrow` provides basic read and write support for the [Apache
Parquet](https://parquet.apache.org/) columnar data file format.

```r
library(arrow)
df <- read_parquet("path/to/file.parquet")
```

Just as you can read, you can write Parquet files:

```r
write_parquet(df, "path/to/different_file.parquet")
```

The `arrow` package also includes a faster and more robust implementation of the
[Feather](https://github.com/wesm/feather) file format, providing `read_feather()` and
`write_feather()`. This implementation depends
on the same underlying C++ library as the Python version does,
resulting in more reliable and consistent behavior across the two languages, as
well as [improved performance](https://wesmckinney.com/blog/feather-arrow-future/).
`arrow` also by default writes the Feather V2 format
([the Arrow IPC file format](https://arrow.apache.org/docs/format/Columnar.html#ipc-file-format)),
which supports a wider range of data types, as well as compression.

For CSV and line-delimited JSON, there are `read_csv_arrow()` and `read_json_arrow()`, respectively.
While `read_csv_arrow()` currently has fewer parsing options for dealing with
every CSV format variation in the wild, for the files it can read, it is
often significantly faster than other R CSV readers, such as
`base::read.csv`, `readr::read_csv`, and `data.table::fread`.


