---
title: "Reading and writing data files"
description: >
  Learn how to read and write CSV, Parquet, and Feather files with `arrow` 
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reading and writing data files}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `arrow` package provides functions for reading single data files in
several common formats. By default, calling any of these functions
returns an R data frame. To return an Arrow Table, set argument
`as_data_frame = FALSE`.

- `read_parquet()`: read a file in Parquet format
- `read_feather()`: read a file in Feather format (the Apache Arrow IPC format)
- `read_delim_arrow()`: read a delimited text file (default delimiter is comma)
- `read_csv_arrow()`: read a comma-separated values (CSV) file
- `read_tsv_arrow()`: read a tab-separated values (TSV) file
- `read_json_arrow()`: read a JSON data file

For writing data to single files, the `arrow` package provides the
following functions, which can be used with both R data frames and 
Arrow Tables:

- `write_parquet()`: write a file in Parquet format
- `write_feather()`: write a file in Feather format
- `write_csv_arrow()`: write a file in CSV format

All these functions can read and write files in the local filesystem or
to cloud storage. For more on cloud storage support in `arrow`, see
`vignette("cloud_storage", package = "arrow")`.

The `arrow` package also supports reading and writing multi-file datasets,
which enable analysis and processing of larger-than-memory data, and provide 
the ability to partition data into smaller chunks without loading the full 
data into memory. For more information on this topic, see 
`vignette("dataset", package = "arrow")`.


## Parquet format

[Apache Parquet](https://parquet.apache.org/) is a popular
choice for storing analytics data; it is a binary format that is 
optimized for reduced file sizes and fast read performance, especially 
for column-based access patterns. The simplest way to read and write
Parquet data using `arrow` is with the `read_parquet()` and 
`write_parquet()` functions. To illustrate this, we'll write the 
`starwars` data included in `dplyr` to a Parquet file, then read it 
back in. First load the `arrow` and `dplyr` packages:

```{r}
library(arrow, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
```

Next we'll write the data frame to a Parquet file located at `file_path`:

```{r}
file_path <- tempfile()
write_parquet(starwars, file_path)
```

The size of a Parquet file is typically much smaller than the corresponding CSV 
file would have been. This is in part due to the use of file compression: by default, 
Parquet files written with the `arrow` package use [Snappy compression](https://google.github.io/snappy/) but other options such as gzip 
are also supported. See `help("write_parquet", package = "arrow")` for more
information.

Having written the Parquet file, we now can read it with `read_parquet()`:

```{r}
read_parquet(file_path)
```

The default is to return a data frame or tibble. If we want an Arrow Table instead, we would set `as_data_frame = FALSE`:

```{r}
read_parquet(file_path, as_data_frame = FALSE)
```

One useful feature of Parquet files is that they store data column-wise, and contain metadata that allow the Parquet readers to skip to the relevant sections of the file. That means it is possible to load only a subset of the columns without reading the complete file. This functionality is supported in `arrow` using the `col_select` argument to `read_parquet()`:

```{r}
read_parquet(file_path, col_select = c("name", "height", "mass"))
```

R object attributes are preserved when writing data to Parquet or
Feather files and when reading those files back into R. This enables
round-trip writing and reading of `sf::sf` objects, R data frames with
with `haven::labelled` columns, and data frame with other custom
attributes. To learn more about how metadata are handled in `arrow`,
see `vignette("metadata", package = "arrow")`.

## Feather format

The Feather file format was developed to provide binary columnar 
serialization for data frames, to make reading and writing data frames 
efficient, and to make sharing data across data analysis languages easy.
It is closely associated with the Apache Arrow project, and development
of the original [Feather](https://github.com/wesm/feather) project has 
now been moved into the Arrow project itself, and you can find the 
detailed specification of version 2 of the Feather format -- officially 
referred to as [the IPC file format](https://arrow.apache.org/docs/format/Columnar.html#ipc-file-format) --
on the Arrow specification page. 

The `write_feather()` function writes version 2 Feather files by default, and supports multiple kinds of file compression. Basic use is shown below:

```{r}
file_path <- tempfile()
write_feather(starwars, file_path)
```

The `read_feather()` function provides a familiar interface for reading feather files:

```{r}
read_feather(file_path)
```

Like the Parquet reader, the Feather reader supports reading a only subset of columns, and can produce Arrow Table output:

```{r}
read_feather(
  file = file_path, 
  col_select = c("name", "height", "mass"), 
  as_data_frame = FALSE
)
```

## CSV format

For CSV and line-delimited JSON, there are `read_csv_arrow()` and `read_json_arrow()`, respectively.
While `read_csv_arrow()` currently has fewer parsing options for dealing with
every CSV format variation in the wild, for the files it can read, it is
often significantly faster than other R CSV readers, such as
`base::read.csv`, `readr::read_csv`, and `data.table::fread`.


## Further reading

- To learn more about cloud storage, see `vignette("cloud_storage", package = "arrow")`.
- To learn more about multi-file datasets, see `vignette("dataset", package = "arrow")`.

