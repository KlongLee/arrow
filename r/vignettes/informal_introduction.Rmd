---
title: Getting started with Apache Arrow and R  
description: >
  An informal introduction of the functioning of Apache Arrow for R users
output: rmarkdown::html_vignette
---


## What is this vignette and is it for you?

This vignette provides an overview of how Arrow works in a plain, non-technical language. It aims at giving some simple and useful intuitions, before diving deeper in the documentation. It is specifically intended for newcomers with a limited background in computer science and hence avoids most technical terms. This vignette assumes that you have some experience with R, that you are familiar with the `dplyr` syntax and that you know how to use a Parquet file.

Some technical points are deliberately simplified to keep things simple, so in case you find an apparent contradiction between this vignette and the rest of the documentation, please trust the documentation rather than this vignette.

## Introducing Apache Arrow

This section introduces the Apache Arrow project.

### What is Apache Arrow?

[Apache `Arrow`](https://arrow.apache.org/) is an *open-source* project that offers two things: 

- Apache Arrow defines a standardized way to organize data in memory (called _Apache Arrow Columnar Format_). You do not need to know much about this _Columnar Format_ to use Arrow with R, except that it is very efficient (processing is fast) and interoperable (meaning for instance that both R and Python can access the same data, without converting the data from one format to another).
- Apache Arrow offers a `C++` implementation of this _Columnar Format_: the `C++` library called `libarrow`.

What about the Arrow R package then? This package simply makes it possible to use the `libarrow` library within `R`. Keep in mind that there are other similar interfaces for using `libarrow` with other programming languages: in Python, Java, Javascript, Julia, and so on. But __no matter what programming language you choose for using Arrow, remember that under the hood you are using exactly the same tool: the C++ libarrow library__.

### What's so special about Arrow?

Arrow has five distinctive features:

- __Columnar Memory Format__: in Arrow, data is organized in columns rather than in rows (hence the "columnar format"). In practice, it means that all values of the first column are stored contiguously in memory, followed by all values of the second column, and so on. This columnar format speeds up data processing. Imagine that you want to calculate the mean of a variable: you can directly access the block of memory containing the entire column and get the result, no matter how many columns you have in your dataset.

- __Easy use with Parquet files__: Arrow is optimized to work well with data stored in Parquet files.

- __Ability to process very large datasets__: Arrow is able to process very large amounts of data, even datasets that are too large to fit in the memory of your computer.

- __Interoperability__: Arrow is designed to be interoperable between several programming languages such as `R`, Python, Java, C++, etc. This means that data can be exchanged between different programming languages without converting the data from one format to another, resulting in significant performance gains.

- __*Lazy Evaluation*__: when you give instructions to Arrow, Arrow stores them but does not run them, unless you explicitly ask it to do so (more on this below).

## What do you need to know to use Arrow with R?

This section describes four important features of `arrow` you should be aware of:

- data is stored in a specific data structure: the `Arrow Table` (quite similar to a standard data.frame);
- you can manipulate Arrow Tables with the usual `dplyr` syntax (verbs `select()`, `filter()`, `mutate()` and so on);
- The data processing is carried out by the Arrow execution engine: `acero`;
- Arrow relies on lazy evaluation: data is processed only when needed.

### What is an `Arrow Table`?

Arrow stores data in a specific data structure: the `Arrow Table`. In an `Arrow Table` object, data is organized in columns rather than rows, according to the Apache Arrow Columnar Format. In practice, you can work `Arrow Tables` pretty much in the same way you work with standard data.frames. The key differences are described in the rest of this vignette.

You can easily convert data to and from `Arrow Tables`. sTo convert a `data.frame` or a `tibble` into an `Arrow Table`, simply use the `as_arrow_table()` function. To convert an `Arrow Table` to a data.frame, use the `collect()` function.

```{r}
library(arrow)
library(dplyr)

# Load data and convert to Arrow Table
iris_arrow <- iris |> as_arrow_table()

# Convert to Arrow Table to a data.frame
iris_df <- iris_arrow |> collect()
```

There are some differences between `Arrow Tables` and standard `data.frames` or `tibbles`.

First difference: if you call the name of a `data.frame` or a `tibble` in the console, the first few rows of data will be displayed. If you do the same operation on an `Arrow Table`, you will get only metadata describing the data (number of rows and columns, column name and type).

```{r}
# Display a data.frame
iris
```

```{r}
# Display an Arrow Table
iris_arrow
```

Second difference: you probably use frequently the `View()` function to have a look at the data store in a data.frame. It is unfortunately not possible (so far) to open an Arrow Table in the same way. You should rather use the `slice_head()` function to extract the first rows of the data, and then use `collect()` to convert it to a data.frame, and then use `View()`.

```{r}
# Display an Arrow Table
iris_extract <- iris_arrow |> slice_head(n = 10) |> collect()
```

### Manipulating `Arrow Tables` with the `dplyr` syntax

The `arrow` package makes it possible to manipulate `Arrow Tables` with the usual `dplyr` syntax (verbs `select()`, `filter()`, `mutate()`, `left_join()`, etc.), __as if__ this table was a standard `data.frame` or `tibble`. It is also possible to use a number of functions from the `tidyverse` packages (such as `stringr` and `lubridate`) to modify `Arrow Table`. This is very convenient in practice: once you know how to use `dplyr` and the `tidyverse`, you can start using `arrow` without having to learn a whole new syntax.

You can see in the following example that manipulating an Arrow Table is almost identical to manipulating a data.frame. The only apparent difference between the two codes is the presence of the `collect()` function at the end of the instructions. The purpose of this `collect()` is explained later, in the paragraph on lazy evaluation.


```r
# Manipulating a data.frame with the dplyr syntax
iris_df |> group_by(Species) |> summarise(Sepal.Width_mean = mean(Sepal.Width))

# Manipulating an Arrow Table with the dplyr syntax
iris_arrow |> group_by(Species) |> summarise(Sepal.Width_mean = mean(Sepal.Width)) |> collect()
```

### The Arrow execution engine: `acero`

There is one essential difference between manipulating a `data.frame` or a `tibble` and manipulating an `Arrow Table`. To understand this difference, you must first understand the __distinction between data manipulation syntax and execution engine__:

- Data manipulation syntax is the set of functions you use to describe the operations you want to perform (calculating averages, making joins...), independently of how these calculations are actually carried out; for instance, the tidyverse syntax and SQL queries are two examples of data manipulation syntaxes;
- the execution engine refers to the way computations are actually performed, independently of how they have been described by the user. Most of the time, the execution engine is not directly accessible to the user.

__The big difference between manipulating a `data.frame` and manipulating an `Arrow Table` lies in the execution engine__: if you manipulate a `data.frame` with the dplyr syntax, then computations are performed by the `dplyr` execution engine; if you manipulate an `Arrow Table` with the dplyr syntax, then computations are performed by a completely different execution engine: the `arrow` execution engine named `acero` and written in C++. It's precisely because `acero` is much more efficient than `dplyr` execution engine that `arrow` is so much faster than `dplyr`.

__This hidden difference on execution engines has an important technical consequence__: when you manipulate an `Arrow Table` with the dplyr syntax, your code needs to be converted into C++ so that the `acero` engine can perform the computations. This conversion is performed automatically and invisibly by the R arrow package, because this package contains the C++ translation of several hundred `tidyverse` functions. For example, the arrow contains the C++ translation of the `filter()` function from `dplyr`, so that `filter()` instructions written in `tidyverse` syntax can be automatically converted into equivalent C++ instructions. The list of _tidyverse_ functions supported by `acero` is available on [this page](https://arrow.apache.org/docs/dev/r/reference/acero.html). Occasionally, however, you may wish to use a function not supported by `acero`. This situation is described in the paragraph "How to use a function not supported by `acero`".

__This is the magic trick you should keep in mind__: it _looks like_ you are writing and running R code, but in fact your code is _invisibly translated into C++ code_ and sent to the `acero` execution engine.

It may happen sometimes that you want to use an R function that the arrow package cannot convert automatically. In technical terms, the function is said to be "not supported by `acero`". This problem can often be solved at a minor cost, see below the paragraph "How to use a function not supported by `acero`".

### Lazy evaluation with Arrow

Arrow has a specific behavior called  _lazy evaluation_: calculations are only performed when they are actually needed. In practice, this means that `arrow` memorizes the instructions you submit, but does not perform any computation until you explicitly ask for it. You can use two functions to ask Arrow to perform computations: `collect()` and `compute()`. There is only one difference between `collect()` and `compute()`, but it's an important one: `collect()` returns the result of processing as a `data.frame`, while `compute()` returns it as an `Arrow Table`.

You might wonder what the point of *lazy evaluation* is. Why not execute each instruction immediately
(this is called *eager evaluation*)? *Lazy evaluation* is useful for performance purposes: when you ask Arrow to execute a series of instructions, Arrow first analyzes this series of instructions, optimizes it (for instance by removing some useless columns) and does all the data processing all at once, often faster than on an instruction-by-instruction basis. This kind of optimization can be very useful, particularly when you process large datasets.

## How do I use Arrow properly?

At first sight, it looks like Arrow can be used in exactly the same way as dplyr (and in fact, this is deliberate!). However, there are a few important differences that you should know about to understand what is going on. This section details three recommendations to use Arrow efficiently:

- Use lazy evaluation correctly;
- Use `compute()` rather than `collect()`;
- Use `open_dataset()` rather than `read_parquet()`.

### How to use lazy evaluation properly

The previous section explained what lazy evaluation is and why it is useful to optimize performance. However, lazy evaluation is not always easy to use, and has limitations that you need to know about.

#### The limitations of lazy evaluation

Lazy evaluation improves performance by minimizing the amount of computations actually needed to execute a series of instructions. With that in mind, you might think that the best way to use Arrow is to write all your data processing in _lazy_ mode (without any `compute()` or `collect()` in the intermediate steps), and do a single `compute()` or `collect()` at the very end of the process, so that all operations are optimized in a single step. If that was true, an ideal process would then look like this:

```{r, eval=FALSE}
# Connect to raw data
data1 <- open_dataset("data1.parquet")
data2 <- open_dataset("data2.parquet")

# A first lazy processing step
intermediate_results1 <- data1 |>
  select(...) |>
  filter(...) |>
  mutate(...)

# A second lazy processing step
intermediate_results2 <- data2 |>
  select(...) |>
  filter(...) |>
  mutate(...)

# Et many other lazy processing steps with many instructions...
# ...
# ...
# ...

# The last processing step
final_results <- intermediate_results8 |>
  left_join(
    intermediate_results9, 
    by = "id"
  ) |>
  compute()
  
write_parquet(final_results, "final_results.parquet")
```

Unfortunately, __reality is not that simple because deferred evaluation has limitations__. 

Indeed, when producing the final result of the previous example, the `compute()` function instructs the `acero` engine to analyze and then execute _all processing at once_ (the previous paragraph gives a detailed example). The `acero` engine may be powerful, but it has its limits and is unable to carry out complex processing in a single operation. For example, `acero` encounters difficulties when linking multiple joins of large tables.

__These limits of deferred evaluation can lead to violent bugs__. When the `acero` engine fails to execute a query that is too complex, the consequences are brutal: `R` prints no error message, the `R` session crashes and you simply have to restart `R` and start all over again. It's therefore essential to structure processing correctly, to take advantage of the benefits of deferred evaluation without touching its limits.




